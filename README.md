# ML_Papers
This Repo is a collection of ML Paper notes

## Table of Contents

- [Introduction](#introduction)
- [Paper List](#paper-list)
- [Contributing](#contributing)

## Introduction

This repository is a collection of notes and summaries for various machine learning papers. It aims to provide a quick reference and understanding of key concepts, methodologies, and findings from important papers in the field of machine learning.

## Paper List

1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    - Summary: Introduce attention mechanism, greatly improve machine translation task.

2. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
    - Summary: Apply pure transformer to vision task and achieve SOTA performance.

3. [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)
    - Summary: Neural network have enough capacity to memorize completely random inputs.

4. [Swin Transformer](https://arxiv.org/abs/2103.14030)

## Contributing

Contributions are welcome! If you have notes on a machine learning paper that you would like to add, please fork the repository, add your notes, and submit a pull request. Make sure to follow the existing format for consistency.
